{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadTwitterData(filePath):\n",
    "    \n",
    "    df = spark.read.json(filePath + '*.gz')\n",
    "    df2 = df.select('body','gnip.matching_rules.tag', \\\n",
    "                    'gnip.matching_rules.value', \\\n",
    "                    'postedTime', 'retweetCount').filter(df.twitter_lang == \"en\").na.drop()\n",
    "    df2 = df2.withColumn('date', df2['postedTime'].cast('date'))\n",
    "    df2 = df2.withColumn('tag', df2['tag'].cast('string')) \\\n",
    "         .withColumn('value', df2['value'].cast('string')) \n",
    "    df2 = df2.withColumnRenamed(\"tag\", \"movieName\")\n",
    "    df2 = df2.withColumnRenamed(\"value\", \"searchPattern\")\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the VADER Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf, avg\n",
    "\n",
    "# sentiment analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# classifying vader scores into bins\n",
    "from pyspark.ml.feature import Bucketizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "def compoundScore(text):\n",
    "    \"\"\"\n",
    "    The proportion of words in a tweet that are classified as neutral\n",
    "\n",
    "    INPUTS:\n",
    "    text = a Spark column of tweets\n",
    "    \n",
    "    OTHER FUNCTIONS AND FILES CALLED BY THIS FUNCTION: \n",
    "    -  vaderSentiment polarity analyzer: imported as SentimentIntensityAnalyzer()\n",
    "    \n",
    "    OBJECTS CREATED WITHIN THIS FUNCTION:\n",
    "    - compoundScores = a Spark SQL function that sums the valence scores of each\n",
    "        words in the lexicon, and normalizes the result to be between \n",
    "        -1 (most extreme negative) and +1 (most extreme positive)\n",
    "    - neutralScores_udf = a column of a Spark DataFrame where each row is the \n",
    "        normalized result from the lexicon\n",
    "    \n",
    "    FILES CREATED BY THIS FUNCTION: None\n",
    "    \n",
    "    RETURNS: \n",
    "    - neutralScores_udf - a column of proportions of neutral words as classified\n",
    "        under VADER    \n",
    "    \"\"\"\n",
    "    \n",
    "    compoundScores = analyzer.polarity_scores(text).get('compound')\n",
    "    return compoundScores\n",
    "\n",
    "compound_udf = udf(compoundScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def returnCompoundScore(dataset, textColumn = 'body' , outputColumn = 'vaderScore'):\n",
    "    \n",
    "    print 'Computing VADER Scores for each tweet'\n",
    "    sentiment = dataset.withColumn(outputColumn, compound_udf(col(textColumn)).cast('Double'))\n",
    "    \n",
    "    return sentiment\n",
    "\n",
    "def vaderClassify(dataset, textColumn, thresholds = [-1.0, -0.5, 0.5, 1.0]):\n",
    "    \n",
    "    \n",
    "#     # return vader score from text as column 'vaderScore'\n",
    "#     outCol  = 'vaderScore'\n",
    "#     sentimentData = returnCompoundScore(dataset, textColumn, outCol)\n",
    "    \n",
    "    print 'Classifying all tweets in to buckets using the cutoffs', thresholds[1], 'and', thresholds [2]\n",
    "    \n",
    "    # classify using thresholds, returns a Classifier\n",
    "    bucketizer = Bucketizer(splits = thresholds, inputCol = \"vaderScore\", outputCol = \"vaderClassifier\")\n",
    "    \n",
    "    print(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1))\n",
    "    \n",
    "    bucketedData = bucketizer.transform(dataset)\n",
    "    return bucketedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Unique Movies to Filter on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uniqueMovies(dataset, movieKey):\n",
    "    # find all unique movie names, some tweets have multiple movies attached \n",
    "    # to them and the names are separated by commas\n",
    "    movies = dataset.select(movieKey).where(~col(movieKey).like('%,%')).distinct().collect()\n",
    "        \n",
    "    # strip the markup to return the name only\n",
    "    moviesUniq = [str(iMovie.movieName[1:-1]) for iMovie in movies]\n",
    "        \n",
    "    return moviesUniq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Movie Level analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def singleMovieTweets(dataset, identifier):\n",
    "    #print 'identifier is', identifier\n",
    "    singleMovie = dataset.filter(dataset.movieName.like('%{0}%' \\\n",
    "                                                .format(identifier)))\n",
    "    return singleMovie\n",
    "    \n",
    "def vaderCountsByClassification(dataset, identifier):\n",
    "    #print 'identifier is', identifier\n",
    "    vaderCounts = dataset.groupby(dataset.date, dataset.vaderClassifier).count()\n",
    "    vaderCounts = vaderCounts.withColumnRenamed(\"count\", \"nTweets\")\n",
    "    vaderCounts = vaderCounts.withColumn('movieName', lit(identifier))\n",
    "    vaderCounts = vaderCounts.orderBy(['date', 'vaderClassifier'], ascending=False)\n",
    "    return vaderCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev, min, max, count\n",
    "\n",
    "def vaderStats(dataset, identifier, vaderCol = 'vaderScore'):\n",
    "    \n",
    "    # aggregate functions\n",
    "    aggStats = [mean, stddev, min, max, count]\n",
    "    aggVariable = [vaderCol] \n",
    "    exprs = [iStat(col(iVariable)) for iStat in aggStats for iVariable in aggVariable]\n",
    "    \n",
    "    # summary stats \n",
    "    dailyStats = dataset.groupby('date').agg(*exprs)\n",
    "    \n",
    "    # rename cols\n",
    "    autoNames  = dailyStats.schema.names\n",
    "    newNames   = [\"date\", \"avgScore\", \"stdDev\", \"minScore\", \"maxScore\", \"totalTweets\"]\n",
    "    \n",
    "    dailyStats = reduce(lambda dailyStats, idx: dailyStats.withColumnRenamed(autoNames[idx], newNames[idx]), \n",
    "                            xrange(len(autoNames)), dailyStats)\n",
    "    dailyStats = dailyStats.withColumn('movieName', lit(identifier))\n",
    "    dailyStats = dailyStats.orderBy(['date'], ascending=False)\n",
    "    \n",
    "    return dailyStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeVaderCounts(dataset, movieList):\n",
    "\n",
    "    for idx, movieName in enumerate(movieList): # for all unique movies\n",
    "        \n",
    "        # get tweets for one movie\n",
    "        indivTweets = singleMovieTweets(dataset, movieList[idx])\n",
    "        # count tweets by type\n",
    "        indivCounts = vaderCountsByClassification(indivTweets, movieList[idx])\n",
    "\n",
    "        if 'allVaderCounts' not in locals() or 'allVaderCounts' in globals():\n",
    "            allVaderCounts = indivCounts\n",
    "        if 'allVaderCounts' in locals() or 'allVaderCounts' in globals():\n",
    "            allVaderCounts = allVaderCounts.union(indivCounts)\n",
    "\n",
    "        print movieName, 'passed Lazy Evaluation at the tweet count stage'\n",
    "    \n",
    "    print 'Converting Count Data to Pandas DF, this may take a while...'\n",
    "    pandasVaderCounts = allVaderCounts.toPandas()\n",
    "\n",
    "    print 'complete!'\n",
    "    return pandasVaderCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeVaderStats(dataset, movieList):\n",
    "\n",
    "    for idx, movieTitle in enumerate(movieList): # for all unique movies\n",
    "\n",
    "        # get tweets for one movie\n",
    "        indivTweets = singleMovieTweets(dataset, movieList[idx])\n",
    "        # count tweets by type\n",
    "        indivStats = vaderStats(indivTweets, movieList[idx])\n",
    "\n",
    "        if 'allVaderStats' not in locals() or 'allVaderStats' in globals():\n",
    "            allVaderStats = indivStats\n",
    "        if 'allVaderStats' in locals() or 'allVaderStats' in globals():\n",
    "            allVaderStats = allVaderStats.union(indivStats)\n",
    "\n",
    "        print movieTitle, 'passed Lazy Evaluation at the daily summary stage'\n",
    "    \n",
    "    print 'Converting daily stats to Pandas DF, this may take a while...'\n",
    "    pandasVaderStats = allVaderStats.toPandas()\n",
    "\n",
    "    print 'complete!'\n",
    "    return pandasVaderStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vaderCounts2csv(dataset, inPath, outPath):\n",
    "    \n",
    "    # make output directory if not created\n",
    "    if not os.path.exists(outPath):\n",
    "        os.makedirs(outPath)\n",
    "    \n",
    "    # save to .csv File\n",
    "    fileName = os.path.basename(os.path.normpath(inPath)) + 'Counts.csv'\n",
    "    outFile  = outPath + fileName\n",
    "    dataset.to_csv(outFile, index=False, encoding='utf-8')\n",
    "    \n",
    "    print 'VADER Counts saved to csv file', outFile\n",
    "    \n",
    "def vaderStats2csv(dataset, inPath, outPath):\n",
    "    \n",
    "    # make output directory if not created\n",
    "    if not os.path.exists(outPath):\n",
    "        os.makedirs(outPath)\n",
    "    \n",
    "    # save to .csv File\n",
    "    fileName = os.path.basename(os.path.normpath(inPath)) + 'Stats.csv'\n",
    "    outFile  = outPath + fileName\n",
    "    dataset.to_csv(outFile, index=False, encoding='utf-8')\n",
    "    \n",
    "    print 'VADER Stats saved to csv file', outFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parseMovieData(filePath, textCol = 'body', thresholds = [-1.0, -0.5, 0.5, 1.0]):\n",
    "    \n",
    "    # TODO: check if zero length files, and clean them out before loading the data\n",
    "    \n",
    "    print 'I am loading the data from ', filePath\n",
    "    # load the data\n",
    "    df = loadTwitterData(filePath)\n",
    "    print 'data loaded ...'\n",
    "    \n",
    "    ## --- Compute Sentiment and Classify --- ##\n",
    "    \n",
    "    # sentiment data\n",
    "    sentimentData = returnCompoundScore(df, textCol)\n",
    "    # compute Classification for each tweet\n",
    "    classifiedData = vaderClassify(sentimentData, textCol, thresholds)\n",
    "    \n",
    "    # identify unique movies\n",
    "    moviesUnique = uniqueMovies(df, 'movieName')\n",
    "    \n",
    "    print 'I found ', len(moviesUnique), ' movies in ', filePath\n",
    "    print 'The movies are:'\n",
    "    print '\\n'.join(str(iMovie) for iMovie in moviesUnique) \n",
    "    \n",
    "    ## --- Tweet Counts by Movie - day --- #\n",
    "    \n",
    "    # compute daily counts for each classification for each movie \n",
    "    vaderCounts = computeVaderCounts(classifiedData, moviesUnique)\n",
    "    \n",
    "    # save vaderCounts to a csv file\n",
    "#     outCounts  = 'out/vaderCounts/'\n",
    "#     vaderCounts2csv(vaderCounts, filePath, outCounts)\n",
    "    \n",
    "    ## --- Summary Stats by Movie-day --- ##\n",
    "    # TODO: compute summary stats per day of the vader output\n",
    "    \n",
    "#     # compute daily counts for each classification for each movie \n",
    "#     vaderStats = computeVaderStats(sentimentData, moviesUnique)\n",
    "    \n",
    "#     # save vaderCounts to a csv file\n",
    "#     outStats  = 'out/vaderStats/'\n",
    "#     vaderStats2csv(vaderStats, filePath, outStats)\n",
    "    \n",
    "    return vaderCounts #, vaderStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a Folder that has multiple movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataPath = '/twitter/movie/**/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am loading the data from  /twitter/movie/**/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-d793fbda293b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparseMovieData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-826f4710fbd7>\u001b[0m in \u001b[0;36mparseMovieData\u001b[0;34m(filePath, textCol, thresholds)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'I am loading the data from '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilePath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadTwitterData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'data loaded ...'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-4441b467e0e2>\u001b[0m in \u001b[0;36mloadTwitterData\u001b[0;34m(filePath)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloadTwitterData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilePath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'*.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'gnip.matching_rules.tag'\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0;34m'gnip.matching_rules.value'\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0;34m'postedTime'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'retweetCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtwitter_lang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'postedTime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda2/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "counts, stats = parseMovieData(dataPath, thresholds = [-1.0, -1.0/3.0, 1.0/3.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vaderOutput.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vaderOutput.movieName.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vaderOutput' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ca1e98e022f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvaderOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vaderOutput' is not defined"
     ]
    }
   ],
   "source": [
    "type(vaderOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a folder that has one movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataPath2 = '/twitter/movie/DeerAntMan/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vaderOutput2 = parseMovieData(dataPath2, thresholds = [-1.0, -1.0/3.0, 1.0/3.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vaderOutput2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vaderOutput2.movieName.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Output (Experimental)\n",
    "\n",
    "shouldn't need this if the spark2pandas2csv is not overly burdensome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql import DataFrameWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vaderCounts.repartition(1).write.format('com.databricks.spark.csv') \\\n",
    "#     .mode(\"overwrite\").option(\"header\", \"true\") \\\n",
    "#     .save('file:///home/lachlan/out/test_save/vaderCount_test.csv')\n",
    "    \n",
    "# vaderCounts.repartition(1).write.csv(path='file:///home/lachlan/out/test_save/vaderTest.csv',\n",
    "#                       header=\"true\", mode=\"overwrite\")\n",
    "vaderOutput2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!ls -a /home/lachlan/out/test_save/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!rm -rf /home/lachlan/out/test_save/vaderTest.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!head /home/lachlan/out/test_save/vaderCount_test.csv"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2 with Spark",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
