{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ant Man: Summary from VADER Sentiment Tool\n",
    "\n",
    "In this notebook we compute the summary data at the daily level for the file 'Antman'\n",
    "\n",
    "We compute:\n",
    "- total tweets per day\n",
    "- Number of positive, negative and neutral tweets per day\n",
    "- Summary stats of sentiment per day: mean, median, std dev, percentiles\n",
    "\n",
    "We save the result as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
      "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/07 22:57:19 WARN Utils: Your hostname, lachlan-tower resolves to a loopback address: 127.0.1.1; using 137.56.70.23 instead (on interface enp0s31f6)\n",
      "24/03/07 22:57:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/07 22:57:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 53048)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark/spark-3.5.0-bin-hadoop3/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark/spark-3.5.0-bin-hadoop3/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/opt/spark/spark-3.5.0-bin-hadoop3/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark/spark-3.5.0-bin-hadoop3/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"spark://lachlan-tower:7077\").appName(\"MovieTweetExplorer\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'sqlContext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqlContext\u001b[49m\u001b[38;5;241m.\u001b[39msetConf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparl.sql.files.ignoreCorruptFiles\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'sqlContext'"
     ]
    }
   ],
   "source": [
    "#spark.sqlContext.setConf(\"sparl.sql.files.ignoreCorruptFiles\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark functions\n",
    "from pyspark.sql.functions import col, udf, avg\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import mean, stddev, min, max, count\n",
    "\n",
    "# sentiment analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# classifying vader scores into bins\n",
    "from pyspark.ml.feature import Bucketizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring the functions we need\n",
    "\n",
    "### Daily tweet counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dailyTweetCount(dataset):\n",
    "    \"\"\"\n",
    "    Counts the number of tweets posted per day for an individual dataset\n",
    "\n",
    "    INPUTS:\n",
    "    dataset = a Spark DataFrame\n",
    "    \n",
    "    OTHER FUNCTIONS AND FILES CALLED BY THIS FUNCTION: \n",
    "    - None\n",
    "    \n",
    "    OBJECTS CREATED WITHIN THIS FUNCTION:\n",
    "    - dailyCounts = Spark DataFrame, a an aggregate count of the number of\n",
    "                    tweets per day\n",
    "    \n",
    "    FILES CREATED BY THIS FUNCTION: None\n",
    "    \n",
    "    RETURNS: \n",
    "    - dailyCounts - a Spark Dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = dataset.withColumn('dateColumn', dataset['postedTime'].cast('date'))\n",
    "    dailyCounts = dataset.groupby(dataset.dateColumn).count()\n",
    "    dailyCounts = dailyCounts.withColumnRenamed(\"count\", \"totalTweets\")\n",
    "    return dailyCounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Tweet Sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def negativeScore(text):\n",
    "    \"\"\"\n",
    "    The proportion of words in a tweet that are classified as negative\n",
    "\n",
    "    INPUTS:\n",
    "    text = a Spark column of tweets\n",
    "    \n",
    "    OTHER FUNCTIONS AND FILES CALLED BY THIS FUNCTION: \n",
    "    -  vaderSentiment polarity analyzer: imported as SentimentIntensityAnalyzer()\n",
    "    \n",
    "    OBJECTS CREATED WITHIN THIS FUNCTION:\n",
    "    - negativeScores = a Spark SQL function that computes the ratio of negative \n",
    "        words in each row of a column\n",
    "    - negativeScores_udf = a column of a Spark DataFrame where each row is the \n",
    "        proportion of negative words\n",
    "    \n",
    "    FILES CREATED BY THIS FUNCTION: None\n",
    "    \n",
    "    RETURNS: \n",
    "    - negativeScores_udf - a column of proportions of negative words as classified\n",
    "        under VADER\n",
    "    \n",
    "    \"\"\"\n",
    "    negativeScores = analyzer.polarity_scores(text).get('neg')\n",
    "    negativeScores_udf = udf(negativeScores).cast('double')\n",
    "    return negativeScores_udf\n",
    "\n",
    "def positiveScore(text):\n",
    "    \"\"\"\n",
    "    The proportion of words in a tweet that are classified as positive\n",
    "\n",
    "    INPUTS:\n",
    "    text = a Spark column of tweets\n",
    "    \n",
    "    OTHER FUNCTIONS AND FILES CALLED BY THIS FUNCTION: \n",
    "    -  vaderSentiment polarity analyzer: imported as SentimentIntensityAnalyzer()\n",
    "    \n",
    "    OBJECTS CREATED WITHIN THIS FUNCTION:\n",
    "    - positiveScores = a Spark SQL function that computes the ratio of positive\n",
    "        words in each row of a column\n",
    "    - postiveScores_udf = a column of a Spark DataFrame where each row is the \n",
    "        proportion of positive words\n",
    "    \n",
    "    FILES CREATED BY THIS FUNCTION: None\n",
    "    \n",
    "    RETURNS: \n",
    "    - positiveScores_udf - a column of proportions of positive words as classified\n",
    "        under VADER\n",
    "    \n",
    "    \"\"\"\n",
    "    positiveScores = analyzer.polarity_scores(text).get('pos')\n",
    "    positiveScores_udf = udf(positiveScores).cast('double')\n",
    "    return positiveScores_udf\n",
    "    \n",
    "def neutralScore(text):\n",
    "    \"\"\"\n",
    "    The proportion of words in a tweet that are classified as neutral\n",
    "\n",
    "    INPUTS:\n",
    "    text = a Spark column of tweets\n",
    "    \n",
    "    OTHER FUNCTIONS AND FILES CALLED BY THIS FUNCTION: \n",
    "    -  vaderSentiment polarity analyzer: imported as SentimentIntensityAnalyzer()\n",
    "    \n",
    "    OBJECTS CREATED WITHIN THIS FUNCTION:\n",
    "    - neutralScores = a Spark SQL function that computes the ratio of neutral\n",
    "        words in each row of a column\n",
    "    - neutralScores_udf = a column of a Spark DataFrame where each row is the \n",
    "        proportion of neutral words\n",
    "    \n",
    "    FILES CREATED BY THIS FUNCTION: None\n",
    "    \n",
    "    RETURNS: \n",
    "    - neutralScores_udf - a column of proportions of neutral words as classified\n",
    "        under VADER\n",
    "    \n",
    "    \"\"\"\n",
    "    neutralScores = analyzer.polarity_scores(text).get('neu')\n",
    "    neutral_udf = udf(neutralScores)\n",
    "    return neutralScores_udf\n",
    "\n",
    "def compoundScore(text):\n",
    "    \"\"\"\n",
    "    The proportion of words in a tweet that are classified as neutral\n",
    "\n",
    "    INPUTS:\n",
    "    text = a Spark column of tweets\n",
    "    \n",
    "    OTHER FUNCTIONS AND FILES CALLED BY THIS FUNCTION: \n",
    "    -  vaderSentiment polarity analyzer: imported as SentimentIntensityAnalyzer()\n",
    "    \n",
    "    OBJECTS CREATED WITHIN THIS FUNCTION:\n",
    "    - compoundScores = a Spark SQL function that sums the valence scores of each\n",
    "        words in the lexicon, and normalizes the result to be between \n",
    "        -1 (most extreme negative) and +1 (most extreme positive)\n",
    "    - neutralScores_udf = a column of a Spark DataFrame where each row is the \n",
    "        normalized result from the lexicon\n",
    "    \n",
    "    FILES CREATED BY THIS FUNCTION: None\n",
    "    \n",
    "    RETURNS: \n",
    "    - neutralScores_udf - a column of proportions of neutral words as classified\n",
    "        under VADER    \n",
    "    \"\"\"\n",
    "    \n",
    "    compoundScores = analyzer.polarity_scores(text).get('compound')\n",
    "    return compoundScores\n",
    "\n",
    "# convert to udfs\n",
    "# negative_udf = udf(negative)\n",
    "# positive_udf = udf(positive)\n",
    "# neutral_udf = udf(neutral)\n",
    "compound_udf = udf(compoundScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnCompoundScore(dataset, textColumn, outputColumn):\n",
    "    sentiment = dataset.withColumn(outputColumn, compound_udf(col(textColumn)).cast('Double'))\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vaderClassify(dataset, textColumn, thresholds):\n",
    "    \n",
    "    # return vader score from text as column 'vaderScore'\n",
    "    outCol  = 'vaderScore'\n",
    "    sentimentData = returnCompoundScore(dataset, textColumn, outCol)\n",
    "    \n",
    "    # classify using thresholds, returns a Classifier\n",
    "    bucketizer = Bucketizer(splits = thresholds, inputCol = \"vaderScore\", outputCol = \"vaderClassifier\")\n",
    "    \n",
    "    print(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1))\n",
    "    \n",
    "    bucketedData = bucketizer.transform(sentimentData)\n",
    "    return bucketedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Sentiment Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vaderStats(dataset, textColumn):\n",
    "    \n",
    "    # get vaderscores\n",
    "    outCol  = 'vaderScore'\n",
    "    sentimentData = returnCompoundScore(dataset, textColumn, outCol)\n",
    "    \n",
    "    # aggregate functions\n",
    "    aggStats = [mean, stddev, min, max, count]\n",
    "    aggVariable = [\"vaderScore\"] \n",
    "    exprs = [iStat(col(iVariable)) for iStat in aggStats for iVariable in aggVariable]\n",
    "    \n",
    "    #dailyStats = dailyTweetCount(sentimentData)\n",
    "    \n",
    "    # summary stats \n",
    "    dailyStats = sentimentData.groupby('date').agg(*exprs)\n",
    "    \n",
    "    # rename cols\n",
    "    autoNames = dailyStats.schema.names\n",
    "    newNames  = [\"date\", \"avgScore\", \"stdDev\", \"minScore\", \"maxScore\", \"totalTweets\"]\n",
    "    \n",
    "    dailyStats = reduce(lambda dailyStats, idx: dailyStats.withColumnRenamed(autoNames[idx], newNames[idx]), \n",
    "                            xrange(len(autoNames)), dailyStats)\n",
    "    \n",
    "    dailyStats  = dailyStats.orderBy(['date'], ascending=False)\n",
    "    \n",
    "    return dailyStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vaderCountsByClassification(dataset, textColumn, thresholds):\n",
    "    \n",
    "    classifiedData = vaderClassify(dataset, textColumn, thresholds)\n",
    "    vaderCounts = classifiedData.groupby(classifiedData.date, classifiedData.vaderClassifier).count()\n",
    "    vaderCounts = vaderCounts.withColumnRenamed(\"count\", \"nTweets\")\n",
    "    vaderCounts= vaderCounts.orderBy(['date', 'vaderClassifier'], ascending=False)\n",
    "    return vaderCounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataPath = '/twitter/movie/DeerAntMan/'\n",
    "#dataPath = \"/home/lachlan/from_zurich/all-twitter-data/twitter-chicago/DeerAntMan/\"\n",
    "dataPath = \"/home/lachlan/from_zurich/all-twitter-data/twitter-chicago/DeerSpectre/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTwitterData(filePath):\n",
    "    \n",
    "    df = spark.read.option(\"mode\", \"DROPMALFORMED\").json(filePath + '*.gz', multiLine=True)\n",
    "    df2 = df.select('body', 'postedTime', 'retweetCount').na.drop()\n",
    "    df2 = df2.withColumn('date', df2['postedTime'].cast('date'))\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/07 22:58:01 WARN TaskSetManager: Stage 1 contains a task of very large size (1461 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = loadTwitterData(dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+----------+\n",
      "|                body|          postedTime|retweetCount|      date|\n",
      "+--------------------+--------------------+------------+----------+\n",
      "|I liked a @YouTub...|2016-01-11T02:50:...|           0|2016-01-11|\n",
      "|RT @BilgeEbiri: G...|2016-02-29T04:20:...|          16|2016-02-29|\n",
      "|RT @JimsTweetings...|2015-10-26T19:20:...|         244|2015-10-26|\n",
      "|It's the premiere...|2015-10-26T19:10:...|           0|2015-10-26|\n",
      "|Watch the New 007...|2015-07-22T07:50:...|           0|2015-07-22|\n",
      "|Sam Smith, James ...|2015-10-26T19:50:...|           0|2015-10-26|\n",
      "|Full-length trail...|2015-07-22T08:00:...|           0|2015-07-22|\n",
      "|Before #SPECTRE w...|2015-10-26T20:00:...|           0|2015-10-26|\n",
      "|I hear George Clo...|2015-10-26T18:10:...|           0|2015-10-26|\n",
      "|RT @JackHoward: T...|2015-10-26T18:00:...|          66|2015-10-26|\n",
      "|RT @taran_adarsh:...|2015-07-22T07:40:...|          51|2015-07-22|\n",
      "|RT @Independent: ...|2015-07-22T07:30:...|          28|2015-07-22|\n",
      "|RT @007: Watch th...|2015-07-22T08:10:...|        3753|2015-07-22|\n",
      "|RT @bectu: We're ...|2015-10-26T17:30:...|           1|2015-10-26|\n",
      "|Won't be long unt...|2015-10-26T23:20:...|           0|2015-10-26|\n",
      "|RT @Spectre_X: @s...|2016-02-29T02:00:...|           1|2016-02-29|\n",
      "|RT @SunnyD_UK: RT...|2015-10-26T21:00:...|         446|2015-10-26|\n",
      "|Okay I liked #SPE...|2015-10-26T23:00:...|           0|2015-10-26|\n",
      "|RT @DanGunner1: S...|2015-10-26T22:50:...|           1|2015-10-26|\n",
      "|RT @007: Watch th...|2015-07-22T08:20:...|        4090|2015-07-22|\n",
      "+--------------------+--------------------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get VADER classifier results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [-1.0, -0.05, 0.05, 1.0]\n",
    "textCol = 'body'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketizer output with 3 buckets\n"
     ]
    }
   ],
   "source": [
    "vaderAnalyzed = vaderClassify(df, textCol, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+----------+----------+---------------+\n",
      "|                body|          postedTime|retweetCount|      date|vaderScore|vaderClassifier|\n",
      "+--------------------+--------------------+------------+----------+----------+---------------+\n",
      "|I liked a @YouTub...|2016-01-11T02:50:...|           0|2016-01-11|    0.4215|            2.0|\n",
      "|RT @BilgeEbiri: G...|2016-02-29T04:20:...|          16|2016-02-29|    0.9593|            2.0|\n",
      "|RT @JimsTweetings...|2015-10-26T19:20:...|         244|2015-10-26|    0.5994|            2.0|\n",
      "|It's the premiere...|2015-10-26T19:10:...|           0|2015-10-26|   -0.1779|            0.0|\n",
      "|Watch the New 007...|2015-07-22T07:50:...|           0|2015-07-22|       0.0|            1.0|\n",
      "|Sam Smith, James ...|2015-10-26T19:50:...|           0|2015-10-26|       0.0|            1.0|\n",
      "|Full-length trail...|2015-07-22T08:00:...|           0|2015-07-22|    0.4767|            2.0|\n",
      "|Before #SPECTRE w...|2015-10-26T20:00:...|           0|2015-10-26|       0.0|            1.0|\n",
      "|I hear George Clo...|2015-10-26T18:10:...|           0|2015-10-26|    0.3612|            2.0|\n",
      "|RT @JackHoward: T...|2015-10-26T18:00:...|          66|2015-10-26|       0.0|            1.0|\n",
      "|RT @taran_adarsh:...|2015-07-22T07:40:...|          51|2015-07-22|       0.0|            1.0|\n",
      "|RT @Independent: ...|2015-07-22T07:30:...|          28|2015-07-22|    0.6908|            2.0|\n",
      "|RT @007: Watch th...|2015-07-22T08:10:...|        3753|2015-07-22|       0.0|            1.0|\n",
      "|RT @bectu: We're ...|2015-10-26T17:30:...|           1|2015-10-26|       0.0|            1.0|\n",
      "|Won't be long unt...|2015-10-26T23:20:...|           0|2015-10-26|    0.7983|            2.0|\n",
      "|RT @Spectre_X: @s...|2016-02-29T02:00:...|           1|2016-02-29|    0.3612|            2.0|\n",
      "|RT @SunnyD_UK: RT...|2015-10-26T21:00:...|         446|2015-10-26|    0.7081|            2.0|\n",
      "|Okay I liked #SPE...|2015-10-26T23:00:...|           0|2015-10-26|    0.5023|            2.0|\n",
      "|RT @DanGunner1: S...|2015-10-26T22:50:...|           1|2015-10-26|       0.0|            1.0|\n",
      "|RT @007: Watch th...|2015-07-22T08:20:...|        4090|2015-07-22|       0.0|            1.0|\n",
      "+--------------------+--------------------+------------+----------+----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vaderAnalyzed.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketizer output with 3 buckets\n"
     ]
    }
   ],
   "source": [
    "vaderClassified = vaderCountsByClassification(df, textCol, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=====================================================>(956 + 16) / 974]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-------+\n",
      "|      date|vaderClassifier|nTweets|\n",
      "+----------+---------------+-------+\n",
      "|2016-05-07|            2.0|     29|\n",
      "|2016-05-07|            1.0|     42|\n",
      "|2016-05-07|            0.0|     25|\n",
      "|2016-05-06|            2.0|     22|\n",
      "|2016-05-06|            1.0|     44|\n",
      "|2016-05-06|            0.0|     25|\n",
      "|2016-05-05|            2.0|     31|\n",
      "|2016-05-05|            1.0|     43|\n",
      "|2016-05-05|            0.0|     19|\n",
      "|2016-05-04|            2.0|     40|\n",
      "|2016-05-04|            1.0|     37|\n",
      "|2016-05-04|            0.0|      9|\n",
      "|2016-05-03|            2.0|     42|\n",
      "|2016-05-03|            1.0|     35|\n",
      "|2016-05-03|            0.0|     12|\n",
      "|2016-05-02|            2.0|     40|\n",
      "|2016-05-02|            1.0|     36|\n",
      "|2016-05-02|            0.0|      9|\n",
      "|2016-05-01|            2.0|     33|\n",
      "|2016-05-01|            1.0|     24|\n",
      "+----------+---------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vaderClassified.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#classifiedData.select(\"vaderClassifier\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Summary stats out of the sentiment index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dailyStatistics = vaderStats(df, 'body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+--------+--------+-----------+\n",
      "|      date|            avgScore|             stdDev|minScore|maxScore|totalTweets|\n",
      "+----------+--------------------+-------------------+--------+--------+-----------+\n",
      "|2016-01-16| 0.17615365853658535| 0.4218355230996798| -0.7152|  0.9022|         41|\n",
      "|2016-01-15| 0.31129318181818183|0.38429921082511137| -0.3818|  0.9474|         44|\n",
      "|2016-01-14|  0.1648294117647059|0.41890397768181553| -0.7118|  0.8475|         51|\n",
      "|2016-01-13|-0.21386315789473684| 0.5240459775864724| -0.7118|  0.9167|         76|\n",
      "|2016-01-12| 0.18567169811320758| 0.2949024723905531| -0.6114|  0.9151|         53|\n",
      "|2016-01-11| 0.19425272727272727|0.35801861520738953| -0.5994|  0.8908|         55|\n",
      "|2016-01-10| 0.11873928571428571|0.41725946219613497| -0.8039|  0.8402|         56|\n",
      "|2016-01-09| 0.14811363636363636|0.40568942757101717| -0.8001|  0.9595|        154|\n",
      "|2016-01-08| 0.03624892703862664|0.43017576841769883| -0.8095|  0.9519|        233|\n",
      "|2016-01-07| 0.17389466666666661|0.37002764575071934| -0.6809|  0.9624|         75|\n",
      "|2016-01-06| 0.23498082191780828|0.42628182307324386| -0.7783|  0.8957|         73|\n",
      "|2016-01-05|  0.1816282828282828|0.37099557977464825| -0.6027|  0.9337|         99|\n",
      "|2016-01-04|  0.2724549180327869|  0.437324487033466| -0.9313|    0.92|        122|\n",
      "|2016-01-03| 0.18491830065359474|0.42362508396947635| -0.8221|  0.9382|        153|\n",
      "|2016-01-02| 0.26260989010989005|0.41242466931240485|  -0.784|  0.9796|        182|\n",
      "|2016-01-01| 0.10199090909090906| 0.4912537656127829| -0.8704|  0.9401|        187|\n",
      "|2015-12-31|  0.2476840909090909|0.40790712287028236|   -0.93|  0.8737|        176|\n",
      "|2015-12-30|  0.1439866666666667| 0.3920462202542482| -0.8885|  0.9471|        180|\n",
      "|2015-12-29| 0.07590697674418603| 0.4835722335879503|  -0.784|  0.9497|        172|\n",
      "|2015-12-28| 0.21340286885245907| 0.3828303202555437| -0.8123|  0.9531|        244|\n",
      "+----------+--------------------+-------------------+--------+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dailyStatistics.show(20)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
