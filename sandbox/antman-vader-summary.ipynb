{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ant Man: Summary from VADER Sentiment Tool\n",
    "\n",
    "In this notebook we compute the summary data at the daily level for the file 'Antman'\n",
    "\n",
    "We compute:\n",
    "- total tweets per day\n",
    "- Number of positive, negative and neutral tweets per day\n",
    "- Summary stats of sentiment per day: mean, median, std dev, percentiles\n",
    "\n",
    "We save the result as a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pyspark functions\n",
    "from pyspark.sql.functions import col, udf, avg\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import mean, stddev, min, max, count\n",
    "\n",
    "# sentiment analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# classifying vader scores into bins\n",
    "from pyspark.ml.feature import Bucketizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring the functions we need\n",
    "\n",
    "### Daily tweet counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dailyTweetCount(dataset):\n",
    "    \"\"\"\n",
    "    Counts the number of tweets posted per day for an individual dataset\n",
    "\n",
    "    INPUTS:\n",
    "    dataset = a Spark DataFrame\n",
    "    \n",
    "    OTHER FUNCTIONS AND FILES CALLED BY THIS FUNCTION: \n",
    "    - None\n",
    "    \n",
    "    OBJECTS CREATED WITHIN THIS FUNCTION:\n",
    "    - dailyCounts = Spark DataFrame, a an aggregate count of the number of\n",
    "                    tweets per day\n",
    "    \n",
    "    FILES CREATED BY THIS FUNCTION: None\n",
    "    \n",
    "    RETURNS: \n",
    "    - dailyCounts - a Spark Dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = dataset.withColumn('dateColumn', dataset['postedTime'].cast('date'))\n",
    "    dailyCounts = dataset.groupby(dataset.dateColumn).count()\n",
    "    dailyCounts = dailyCounts.withColumnRenamed(\"count\", \"totalTweets\")\n",
    "    return dailyCounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Tweet Sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def negativeScore(text):\n",
    "    \"\"\"\n",
    "    The proportion of words in a tweet that are classified as negative\n",
    "\n",
    "    INPUTS:\n",
    "    text = a Spark column of tweets\n",
    "    \n",
    "    OTHER FUNCTIONS AND FILES CALLED BY THIS FUNCTION: \n",
    "    -  vaderSentiment polarity analyzer: imported as SentimentIntensityAnalyzer()\n",
    "    \n",
    "    OBJECTS CREATED WITHIN THIS FUNCTION:\n",
    "    - negativeScores = a Spark SQL function that computes the ratio of negative \n",
    "        words in each row of a column\n",
    "    - negativeScores_udf = a column of a Spark DataFrame where each row is the \n",
    "        proportion of negative words\n",
    "    \n",
    "    FILES CREATED BY THIS FUNCTION: None\n",
    "    \n",
    "    RETURNS: \n",
    "    - negativeScores_udf - a column of proportions of negative words as classified\n",
    "        under VADER\n",
    "    \n",
    "    \"\"\"\n",
    "    negativeScores = analyzer.polarity_scores(text).get('neg')\n",
    "    negativeScores_udf = udf(negativeScores).cast('double')\n",
    "    return negativeScores_udf\n",
    "\n",
    "def positiveScore(text):\n",
    "    \"\"\"\n",
    "    The proportion of words in a tweet that are classified as positive\n",
    "\n",
    "    INPUTS:\n",
    "    text = a Spark column of tweets\n",
    "    \n",
    "    OTHER FUNCTIONS AND FILES CALLED BY THIS FUNCTION: \n",
    "    -  vaderSentiment polarity analyzer: imported as SentimentIntensityAnalyzer()\n",
    "    \n",
    "    OBJECTS CREATED WITHIN THIS FUNCTION:\n",
    "    - positiveScores = a Spark SQL function that computes the ratio of positive\n",
    "        words in each row of a column\n",
    "    - postiveScores_udf = a column of a Spark DataFrame where each row is the \n",
    "        proportion of positive words\n",
    "    \n",
    "    FILES CREATED BY THIS FUNCTION: None\n",
    "    \n",
    "    RETURNS: \n",
    "    - positiveScores_udf - a column of proportions of positive words as classified\n",
    "        under VADER\n",
    "    \n",
    "    \"\"\"\n",
    "    positiveScores = analyzer.polarity_scores(text).get('pos')\n",
    "    positiveScores_udf = udf(positiveScores).cast('double')\n",
    "    return positiveScores_udf\n",
    "    \n",
    "def neutralScore(text):\n",
    "    \"\"\"\n",
    "    The proportion of words in a tweet that are classified as neutral\n",
    "\n",
    "    INPUTS:\n",
    "    text = a Spark column of tweets\n",
    "    \n",
    "    OTHER FUNCTIONS AND FILES CALLED BY THIS FUNCTION: \n",
    "    -  vaderSentiment polarity analyzer: imported as SentimentIntensityAnalyzer()\n",
    "    \n",
    "    OBJECTS CREATED WITHIN THIS FUNCTION:\n",
    "    - neutralScores = a Spark SQL function that computes the ratio of neutral\n",
    "        words in each row of a column\n",
    "    - neutralScores_udf = a column of a Spark DataFrame where each row is the \n",
    "        proportion of neutral words\n",
    "    \n",
    "    FILES CREATED BY THIS FUNCTION: None\n",
    "    \n",
    "    RETURNS: \n",
    "    - neutralScores_udf - a column of proportions of neutral words as classified\n",
    "        under VADER\n",
    "    \n",
    "    \"\"\"\n",
    "    neutralScores = analyzer.polarity_scores(text).get('neu')\n",
    "    neutral_udf = udf(neutralScores)\n",
    "    return neutralScores_udf\n",
    "\n",
    "def compoundScore(text):\n",
    "    \"\"\"\n",
    "    The proportion of words in a tweet that are classified as neutral\n",
    "\n",
    "    INPUTS:\n",
    "    text = a Spark column of tweets\n",
    "    \n",
    "    OTHER FUNCTIONS AND FILES CALLED BY THIS FUNCTION: \n",
    "    -  vaderSentiment polarity analyzer: imported as SentimentIntensityAnalyzer()\n",
    "    \n",
    "    OBJECTS CREATED WITHIN THIS FUNCTION:\n",
    "    - compoundScores = a Spark SQL function that sums the valence scores of each\n",
    "        words in the lexicon, and normalizes the result to be between \n",
    "        -1 (most extreme negative) and +1 (most extreme positive)\n",
    "    - neutralScores_udf = a column of a Spark DataFrame where each row is the \n",
    "        normalized result from the lexicon\n",
    "    \n",
    "    FILES CREATED BY THIS FUNCTION: None\n",
    "    \n",
    "    RETURNS: \n",
    "    - neutralScores_udf - a column of proportions of neutral words as classified\n",
    "        under VADER    \n",
    "    \"\"\"\n",
    "    \n",
    "    compoundScores = analyzer.polarity_scores(text).get('compound')\n",
    "    return compoundScores\n",
    "\n",
    "# convert to udfs\n",
    "# negative_udf = udf(negative)\n",
    "# positive_udf = udf(positive)\n",
    "# neutral_udf = udf(neutral)\n",
    "compound_udf = udf(compoundScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def returnCompoundScore(dataset, textColumn, outputColumn):\n",
    "    sentiment = dataset.withColumn(outputColumn, compound_udf(col(textColumn)).cast('Double'))\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vaderClassify(dataset, textColumn, thresholds):\n",
    "    \n",
    "    # return vader score from text as column 'vaderScore'\n",
    "    outCol  = 'vaderScore'\n",
    "    sentimentData = returnCompoundScore(dataset, textColumn, outCol)\n",
    "    \n",
    "    # classify using thresholds, returns a Classifier\n",
    "    bucketizer = Bucketizer(splits = thresholds, inputCol = \"vaderScore\", outputCol = \"vaderClassifier\")\n",
    "    \n",
    "    print(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1))\n",
    "    \n",
    "    bucketedData = bucketizer.transform(sentimentData)\n",
    "    return bucketedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Sentiment Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vaderStats(dataset, textColumn):\n",
    "    \n",
    "    # get vaderscores\n",
    "    outCol  = 'vaderScore'\n",
    "    sentimentData = returnCompoundScore(dataset, textColumn, outCol)\n",
    "    \n",
    "    # aggregate functions\n",
    "    aggStats = [mean, stddev, min, max, count]\n",
    "    aggVariable = [\"vaderScore\"] \n",
    "    exprs = [iStat(col(iVariable)) for iStat in aggStats for iVariable in aggVariable]\n",
    "    \n",
    "    #dailyStats = dailyTweetCount(sentimentData)\n",
    "    \n",
    "    # summary stats \n",
    "    dailyStats = sentimentData.groupby('date').agg(*exprs)\n",
    "    \n",
    "    # rename cols\n",
    "    autoNames = dailyStats.schema.names\n",
    "    newNames  = [\"date\", \"avgScore\", \"stdDev\", \"minScore\", \"maxScore\", \"totalTweets\"]\n",
    "    \n",
    "    dailyStats = reduce(lambda dailyStats, idx: dailyStats.withColumnRenamed(autoNames[idx], newNames[idx]), \n",
    "                            xrange(len(autoNames)), dailyStats)\n",
    "    \n",
    "    dailyStats  = dailyStats.orderBy(['date'], ascending=False)\n",
    "    \n",
    "    return dailyStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vaderCountsByClassification(dataset, textColumn, thresholds):\n",
    "    \n",
    "    classifiedData = vaderClassify(dataset, textColumn, thresholds)\n",
    "    vaderCounts = classifiedData.groupby(classifiedData.date, classifiedData.vaderClassifier).count()\n",
    "    vaderCounts = vaderCounts.withColumnRenamed(\"count\", \"nTweets\")\n",
    "    vaderCounts= vaderCounts.orderBy(['date', 'vaderClassifier'], ascending=False)\n",
    "    return vaderCounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataPath = '/twitter/movie/DeerAntMan/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadTwitterData(filePath):\n",
    "    \n",
    "    df = spark.read.json(filePath + '*.gz')\n",
    "    df2 = df.select('body', 'postedTime', 'retweetCount').na.drop()\n",
    "    df2 = df2.withColumn('date', df2['postedTime'].cast('date'))\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = loadTwitterData(dataPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get VADER classifier results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thresholds = [-1.0, -0.5, 0.5, 1.0]\n",
    "textCol = 'body'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vaderAnalyzed = vaderClassify(df2, textCol, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vaderAnalyzed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketizer output with 3 buckets\n"
     ]
    }
   ],
   "source": [
    "vaderClassified = vaderCountsByClassification(df, textCol, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-------+\n",
      "|      date|vaderClassifier|nTweets|\n",
      "+----------+---------------+-------+\n",
      "|2016-01-16|            2.0|     10|\n",
      "|2016-01-16|            1.0|     28|\n",
      "|2016-01-16|            0.0|      3|\n",
      "|2016-01-15|            2.0|     16|\n",
      "|2016-01-15|            1.0|     28|\n",
      "|2016-01-14|            2.0|     18|\n",
      "|2016-01-14|            1.0|     30|\n",
      "|2016-01-14|            0.0|      3|\n",
      "|2016-01-13|            2.0|      9|\n",
      "|2016-01-13|            1.0|     32|\n",
      "|2016-01-13|            0.0|     35|\n",
      "|2016-01-12|            2.0|      5|\n",
      "|2016-01-12|            1.0|     47|\n",
      "|2016-01-12|            0.0|      1|\n",
      "|2016-01-11|            2.0|     11|\n",
      "|2016-01-11|            1.0|     42|\n",
      "|2016-01-11|            0.0|      2|\n",
      "|2016-01-10|            2.0|     10|\n",
      "|2016-01-10|            1.0|     39|\n",
      "|2016-01-10|            0.0|      7|\n",
      "+----------+---------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vaderClassified.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#classifiedData.select(\"vaderClassifier\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Summary stats out of the sentiment index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dailyStatistics = vaderStats(df, 'body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+--------+--------+-----------+\n",
      "|      date|            avgScore|             stdDev|minScore|maxScore|totalTweets|\n",
      "+----------+--------------------+-------------------+--------+--------+-----------+\n",
      "|2016-01-16| 0.17615365853658535| 0.4218355230996798| -0.7152|  0.9022|         41|\n",
      "|2016-01-15| 0.31129318181818183|0.38429921082511137| -0.3818|  0.9474|         44|\n",
      "|2016-01-14|  0.1648294117647059|0.41890397768181553| -0.7118|  0.8475|         51|\n",
      "|2016-01-13|-0.21386315789473684| 0.5240459775864724| -0.7118|  0.9167|         76|\n",
      "|2016-01-12| 0.18567169811320758| 0.2949024723905531| -0.6114|  0.9151|         53|\n",
      "|2016-01-11| 0.19425272727272727|0.35801861520738953| -0.5994|  0.8908|         55|\n",
      "|2016-01-10| 0.11873928571428571|0.41725946219613497| -0.8039|  0.8402|         56|\n",
      "|2016-01-09| 0.14811363636363636|0.40568942757101717| -0.8001|  0.9595|        154|\n",
      "|2016-01-08| 0.03624892703862664|0.43017576841769883| -0.8095|  0.9519|        233|\n",
      "|2016-01-07| 0.17389466666666661|0.37002764575071934| -0.6809|  0.9624|         75|\n",
      "|2016-01-06| 0.23498082191780828|0.42628182307324386| -0.7783|  0.8957|         73|\n",
      "|2016-01-05|  0.1816282828282828|0.37099557977464825| -0.6027|  0.9337|         99|\n",
      "|2016-01-04|  0.2724549180327869|  0.437324487033466| -0.9313|    0.92|        122|\n",
      "|2016-01-03| 0.18491830065359474|0.42362508396947635| -0.8221|  0.9382|        153|\n",
      "|2016-01-02| 0.26260989010989005|0.41242466931240485|  -0.784|  0.9796|        182|\n",
      "|2016-01-01| 0.10199090909090906| 0.4912537656127829| -0.8704|  0.9401|        187|\n",
      "|2015-12-31|  0.2476840909090909|0.40790712287028236|   -0.93|  0.8737|        176|\n",
      "|2015-12-30|  0.1439866666666667| 0.3920462202542482| -0.8885|  0.9471|        180|\n",
      "|2015-12-29| 0.07590697674418603| 0.4835722335879503|  -0.784|  0.9497|        172|\n",
      "|2015-12-28| 0.21340286885245907| 0.3828303202555437| -0.8123|  0.9531|        244|\n",
      "+----------+--------------------+-------------------+--------+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dailyStatistics.show(20)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2 with Spark",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
